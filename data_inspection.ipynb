{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python373jvsc74a57bd03e858b1de7d36d2468595b407a3d6b14503c8ea766a83ef0f1643d341059c774",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nibabel as nib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = pd.read_csv('data/hemorrhage_diagnosis_raw_ct.csv')\n",
    "\n",
    "h_start_idx = 2\n",
    "h_end_idx = 7\n",
    "\n",
    "some = []\n",
    "\n",
    "ambiguous = []\n",
    "\n",
    "#foo = True\n",
    "for row_idx in range(patients.shape[0]):\n",
    "    hemorrhage_row = patients.iloc[row_idx,h_start_idx:h_end_idx]\n",
    "    hemorrhage_sum = hemorrhage_row.sum()\n",
    "    if hemorrhage_sum > 1 or patients.iloc[row_idx, 0] == 51:\n",
    "        ambiguous.append(patients.iloc[row_idx,:])\n",
    "    if hemorrhage_sum > 0:\n",
    "        some.append(patients.iloc[row_idx,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(ambiguous)\n",
    "df.iloc[:,h_start_idx:h_end_idx].sum(1)\n",
    "some_df = pd.DataFrame(some)\n",
    "n_total = some_df.iloc[:,h_start_idx:h_end_idx].shape[0]\n",
    "n_ambiguous = df.iloc[:,h_start_idx:h_end_idx].shape[0]\n",
    "n_unambiguous = n_total - n_ambiguous\n",
    "print(\"Number of unambiguous slices: \", n_unambiguous )\n",
    "print(\"Number of ambiguous slices: \", n_ambiguous )\n",
    "print(\"Total slices with at least one lesion: \", n_total )\n",
    "\n",
    "print(\"Patients with multiple hemorrhages:\", df['PatientNumber'].unique().shape[0])\n",
    "print(\"Nof Patients with any hemorrahge:\", some_df['PatientNumber'].unique().shape[0])\n",
    "print(\"Patients with any hemorrhage:\", some_df['PatientNumber'].unique())\n",
    "print(df['PatientNumber'].unique())\n",
    "\n",
    "none = patients[~patients['PatientNumber'].isin(some_df['PatientNumber'])]['PatientNumber'].unique()\n",
    "print(f\"No lesions: {none}\")\n",
    "print(patients['PatientNumber'].unique().shape[0], none.shape[0], some_df['PatientNumber'].unique().shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import plot_utils\n",
    "import torch\n",
    "import networks.unet2D.utils as utils\n",
    "color_map = torch.tensor([\n",
    "            [0,0,0], # bg: black\n",
    "            [0,1.0,0], # green, IVH\n",
    "            [1.0,0,0], # red, IPH\n",
    "            [0,0,1.0], # blue, SAH\n",
    "            [0.0,0,0.5], # EDH, magenta?\n",
    "            [0,0.5,0.9] # SDH, cyan?\n",
    "    ])\n",
    "\n",
    "def plot(patient_data):\n",
    "    N = patient_data.shape[0]\n",
    "    print(patient_data.shape)\n",
    "    if N > 100:\n",
    "        print(\"!!! TOO MUCH DATA TO SHOW; ABORTING !!!\")\n",
    "        return\n",
    "\n",
    "    patient_numbers = patient_data['PatientNumber'].unique()\n",
    "    counter = 0\n",
    "    for patient_number in patient_numbers:\n",
    "        print(patient_number)\n",
    "        \n",
    "            \n",
    "        slices = patient_data[patient_data.PatientNumber == patient_number].SliceNumber - 1\n",
    "        p_str = f'{patient_number}'\n",
    "        if len(p_str) == 2:\n",
    "            p_str = '0'+p_str\n",
    "\n",
    "        img = nib.load(f'data/ct_scans/{p_str}.nii')\n",
    "        img_data = img.get_fdata().clip(0, 150)\n",
    "\n",
    "        mask_img = nib.load(f'data/multiclass_mask/{p_str}.nii')\n",
    "        mask_img\n",
    "        #mask_img = nib.load(f'data/masks/{p_str}.nii')\n",
    "        mask_data = mask_img.get_fdata()#.clip(0, 150)\n",
    "\n",
    "        for i in slices:\n",
    "            \n",
    "            mask_img = Image.fromarray(mask_data[:,:,i])\n",
    "            src_img = Image.fromarray(img_data[:,:,i])\n",
    "            \n",
    "            mask = np.array(mask_img)\n",
    "            src = np.array(src_img)\n",
    "            if (mask.sum() > 0):\n",
    "                #plt.imshow(mask, cmap='gray', origin='bottom')\n",
    "                #plt.show()\n",
    "                #plt.imshow(src, cmap='gray', origin='bottom')\n",
    "                #plt.show()\n",
    "                #plot_utils.plot_grid(mask,)\n",
    "                ct = torch.tensor((img_data.clip(0,80) / 80)[:,:,i])\n",
    "                mask_tensor = torch.tensor(mask_data[:,:,i])\n",
    "                #plot_utils.plot_masked(ct, mask)\n",
    "                if patient_number == 51 and i == 25:\n",
    "                    print(mask.max(), mask_data[:,:,i].max())\n",
    "\n",
    "                    print(mask_tensor.shape)\n",
    "                    print(ct.unsqueeze(0).unsqueeze(0).shape)\n",
    "                    mask_tensor = mask_tensor#.unsqueeze(0).unsqueeze(0)\n",
    "                    print(\"class max\", mask_tensor.min(), mask_tensor.max())\n",
    "                    print(mask_tensor)\n",
    "                    mask_tensor = mask_tensor.long()\n",
    "\n",
    "                    max_idx = torch.argmax(mask_tensor)\n",
    "\n",
    "                    mask_tensor = utils.labels_to_rgb_batched(mask_tensor.permute(0,2,3,1), color_map)\n",
    "                    mask_tensor = mask_tensor.permute(0,3,1,2)\n",
    "                    ct = ct.unsqueeze(0).unsqueeze(0).repeat(1,3,1,1)\n",
    "\n",
    "                    print(mask_tensor.shape, ct.shape)\n",
    "                    utils.save_overlay_grid(ct, mask_tensor, mask_tensor, 'ct_class_test.png')\n",
    "            counter += 1\n",
    "    print(counter)\n",
    "\n",
    "plot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plot_utils\n",
    "import torch\n",
    "from nibabel.testing import data_path\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "patient_ids = patients.PatientNumber.unique()\n",
    "emptied_slices = 0\n",
    "for k, pid in enumerate(patient_ids):\n",
    "    p_str = str(pid)\n",
    "    if len(p_str) == 2:\n",
    "        p_str = '0'+p_str\n",
    "\n",
    "    print(p_str)\n",
    "\n",
    "    modded_mask_save_path = f'data/multiclass_mask/{p_str}.nii'\n",
    "    modded_img_save_path = f'data/ct_scans/{p_str}.nii'\n",
    "    mask = nib.load(f'data/masks/{p_str}.nii')\n",
    "    mask_data = mask.get_fdata()\n",
    "\n",
    "    img = nib.load(f'data/ct_scans_backup/{p_str}.nii')\n",
    "    img_data = img.get_fdata()\n",
    "    modded_mask_data = np.zeros(mask_data.shape, dtype=np.uint8)\n",
    "    #modded_img_data = np.zeros(img_data.shape)\n",
    "\n",
    "    for i in range(mask_data.shape[2]):\n",
    "        slice_annotation = patients[patients['PatientNumber'] == pid].iloc[i, h_start_idx:h_end_idx]\n",
    "        n = slice_annotation.sum()\n",
    "        #print(pid, i, n)\n",
    "        if n == 1:\n",
    "            # print(f\"slice {i+1} has {n} bleeding type(s). Bleeding type(s): {slice_annotation}\")\n",
    "            class_id = np.array(slice_annotation).nonzero()[0][0] + 1\n",
    "\n",
    "            #if pid == 51:\n",
    "            #    print(class_id)\n",
    "            modded_mask_data[:,:,i][mask_data[:,:,i] > 0] = class_id\n",
    "        elif n > 1:\n",
    "            # Setting the image to zero to prevent false negatives in training. Other possible approaches:\n",
    "            # - randomize one of the classes for the mask (not ideal either)\n",
    "            # - obtain proper unambiguous multi-class masks (ideal)\n",
    "            img_data[:,:,i] = np.full((img_data.shape[0], img_data.shape[1]), fill_value=img_data.min())\n",
    "            emptied_slices += 1\n",
    "\n",
    "    #if pid == 51:\n",
    "    #    print(modded_mask_data)\n",
    "\n",
    "    modded_mask = nib.Nifti1Image(modded_mask_data, affine=mask.affine, header=mask.header)\n",
    "    nib.save(modded_mask, modded_mask_save_path)\n",
    "\n",
    "    modded_img = nib.Nifti1Image(img_data, affine=img.affine, header=img.header)\n",
    "    nib.save(modded_img, modded_img_save_path)\n",
    "\n",
    "print(f\"Emptied {emptied_slices} slices.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([\n",
    "    [\n",
    "        [1,1,1],\n",
    "        [1,1,1],\n",
    "        [1,1,1]\n",
    "    ],\n",
    "    [\n",
    "        [0,0,0],\n",
    "        [1,1,0],\n",
    "        [0,0,0]\n",
    "    ]\n",
    "])\n",
    "\n",
    "modded_A = np.zeros(A.shape)\n",
    "print(A.shape)\n",
    "print(modded_A.shape)\n",
    "modded_A[0][A[0] > 0] = 10\n",
    "modded_A[1][A[1] > 0] = 20\n",
    "\n",
    "modded_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, pid in enumerate(patient_ids):\n",
    "#     p_str = str(pid)\n",
    "#     if len(p_str) == 2:\n",
    "#         p_str = '0'+p_str\n",
    "# \n",
    "#     print(p_str)\n",
    "# \n",
    "#     mask = nib.load(f'data/multiclass_mask/{p_str}.nii')\n",
    "#     mask_data = mask.get_fdata()\n",
    "# \n",
    "#     for i in range(mask_data.shape[2]):\n",
    "#         slice_annotation = patients[patients['PatientNumber'] == pid].iloc[i, h_start_idx:h_end_idx]\n",
    "#         n = slice_annotation.sum()\n",
    "#         if n > 1:\n",
    "#             plt.hist(mask_data[:,:,i])\n",
    "#             plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from shutil import copyfile\n",
    "from tqdm import tqdm\n",
    "patients_with_some_hematoma = np.array(some_df['PatientNumber'].unique())\n",
    "\n",
    "all_patient_ids = patients.PatientNumber.unique()\n",
    "print(\"All patients: \\n\", all_patient_ids)\n",
    "\n",
    "np.random.seed(123)\n",
    "n_val = 3\n",
    "n_test = 3\n",
    "\n",
    "k = 5\n",
    "\n",
    "validation_folds = []\n",
    "test_folds = []\n",
    "train_folds = []\n",
    "\n",
    "test_fold = np.random.choice(patients_with_some_hematoma, n_test, replace=False) # Same test data for all folds for now\n",
    "\n",
    "print(\"Creating folds...\")\n",
    "for i in range(k):\n",
    "    post_test_selection_patients = patients_with_some_hematoma[np.isin(patients_with_some_hematoma, test_fold, invert=True)]\n",
    "    validation_fold = np.random.choice(post_test_selection_patients, n_val, replace=False)\n",
    "    #split_fold = np.split(fold, 2)\n",
    "    #validation_fold, test_fold = split_fold[0], split_fold[1]\n",
    "    already_reserved_data = np.concatenate((validation_fold, test_fold), axis=0)\n",
    "    train_fold = all_patient_ids[np.isin(all_patient_ids, already_reserved_data, invert=True)]\n",
    "    \n",
    "    validation_folds.append(validation_fold)\n",
    "    test_folds.append(test_fold)\n",
    "    train_folds.append(train_fold)\n",
    "    \n",
    "print(\"Folds created!\")\n",
    "\n",
    "#[ print(fold) for fold in folds]\n",
    "print(\"Validation folds: \\n\", validation_folds)\n",
    "print(\"Test folds: \\n\", test_folds)\n",
    "print(\"Train folds: \\n\", train_folds)\n",
    "\n",
    "def create_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "def copy_files_in_fold(source_directory, target_directory, fold):\n",
    "    for patient_number in tqdm(fold):#zip(validation_fold, test_fold, train_folds):\n",
    "        p_str = f'{patient_number}'\n",
    "        if len(p_str) == 2:\n",
    "            p_str = '0'+p_str\n",
    "        \n",
    "        old_file = f'{source_directory}/{p_str}.nii'\n",
    "        new_file = f'{target_directory}/{p_str}.nii'\n",
    "\n",
    "        #print(\"\")\n",
    "        #print(\"old: \", old_file, \" target: \", new_file)\n",
    "        copyfile(old_file, new_file)\n",
    "\n",
    "def copy_files_in_folds(base_directory, validation_folds, test_folds, train_folds):\n",
    "    for i, (validation_fold, test_fold, train_fold) in enumerate(zip(validation_folds, test_folds, train_folds)):\n",
    "        directory = f'{base_directory}/fold_{i}'\n",
    "\n",
    "        create_dir(directory)\n",
    "\n",
    "        val_directory = f'{directory}/validation'\n",
    "        test_directory = f'{directory}/test'\n",
    "        train_directory = f'{directory}/train'\n",
    "\n",
    "        create_dir(val_directory)\n",
    "        create_dir(test_directory)\n",
    "        create_dir(train_directory)\n",
    "\n",
    "        print(\"val test:\")\n",
    "        print(validation_fold, test_fold)\n",
    "        copy_files_in_fold(base_directory, val_directory, validation_fold)\n",
    "        copy_files_in_fold(base_directory, test_directory, test_fold)\n",
    "        copy_files_in_fold(base_directory, train_directory, train_fold)\n",
    "\n",
    "base_directory = 'data/multiclass_mask'\n",
    "copy_files_in_folds(base_directory, validation_folds, test_folds, train_folds)\n",
    "\n",
    "base_directory = 'data/ct_scans'\n",
    "copy_files_in_folds(base_directory, validation_folds, test_folds, train_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_files_in_folds('data/ct_scans', validation_folds, test_folds, train_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "source_directory = 'E:\\\\kaggle\\\\rsna-intracranial-hemorrhage-detection\\\\nifti_down_sampled_train\\\\'\n",
    "target_directory = 'E:\\\\kaggle\\\\rsna-intracranial-hemorrhage-detection\\\\nifti_down_sampled_test\\\\'\n",
    "train_files = listdir(source_directory)\n",
    "test_files = listdir(target_directory)\n",
    "N_test = 5000\n",
    "\n",
    "print(f'About to move {N_test} files. Source dir: {len(train_files)} file(s). Target dir: {len(test_files)} file(s).')\n",
    "del test_files\n",
    "\n",
    "def move_file(source_directory, target_directory, file_name):\n",
    "    source_path = os.path.join(source_directory, file_name)\n",
    "    target_path = os.path.join(target_directory, file_name)\n",
    "    shutil.move(source_path, target_path)\n",
    "\n",
    "def move_files(source_directory, target_directory, files, indices_to_move):\n",
    "    for index_to_move in tqdm(indices_to_move):\n",
    "        file_name = files[index_to_move]\n",
    "        move_file(source_directory, target_directory, file_name)\n",
    "\n",
    "    print(f'\\nMoved {len(indices_to_move)} files.')\n",
    "\n",
    "possible_indices = np.array(list(range(0,len(files))))\n",
    "test_indices = np.random.choice(possible_indices, N_test, replace=False)\n",
    "assert(np.unique(test_indices).shape == test_indices.shape)\n",
    "\n",
    "move_files(\n",
    "    source_directory=source_directory,\n",
    "    target_directory=target_directory,\n",
    "    files=train_files,\n",
    "    indices_to_move=test_indices\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "def write_list(root, file_names, N):\n",
    "    with open(f'{root}/train_{N}.txt', 'w') as f:\n",
    "        for f_n in file_names:\n",
    "            f.write(f'{f_n}\\n')\n",
    "\n",
    "def select(file_names, N):\n",
    "    return np.random.choice(file_names, N, replace=False)\n",
    "\n",
    "def partition(root, file_names, Ns):\n",
    "    file_names_left = file_names\n",
    "\n",
    "    partitions = []\n",
    "    for N in tqdm(Ns):\n",
    "        file_names_left = select(file_names_left, N)\n",
    "        write_list(root, file_names_left, N)\n",
    "        partitions.append(file_names_left)\n",
    "\n",
    "    partitions = list(reversed(partitions))\n",
    "    for i, p in enumerate(partitions):\n",
    "        all_larger = partitions[i+1:]\n",
    "        assert(len(p) == len(np.unique(p)))\n",
    "        for larger in all_larger:\n",
    "            larger = np.array(larger)\n",
    "            p = np.array(p)\n",
    "            smaller_is_fully_included_in_larger = np.all(np.isin(p, larger))\n",
    "            assert(smaller_is_fully_included_in_larger)\n",
    "\n",
    "root = './data/experiment_data'\n",
    "file_names = sorted(listdir('E:\\\\kaggle\\\\rsna-intracranial-hemorrhage-detection\\\\nifti_down_sampled\\\\'))\n",
    "partitions = [10000, 8000, 5000, 4000, 2000, 1000, 500, 100]\n",
    "partition(root, file_names, partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array(['foo', 'bar', 'barr'])\n",
    "B = np.array(['foo', 'bar'])\n",
    "\n",
    "np.alltrue(np.isin(B, A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from networks.dataset.ct import CtIchDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "# TODO: 0.8 negative sampling, update in real dataset?\n",
    "# TODO: Don't use these directly in training, since it contains information from the test set\n",
    "n_classes = 6\n",
    "device = torch.device('cuda:0')\n",
    "dataset = CtIchDataset(img_dir='../backup/data/all_scans/', mask_dir='../backup/data/all_masks/', N=-1, empty_mask_discard_probability=0.8, num_threads=1, resize=0.5, n_classes=6)\n",
    "loader = DataLoader(dataset, batch_size=8, shuffle=False, pin_memory=True)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    counts = torch.zeros((n_classes,)).long().to(device)\n",
    "\n",
    "    for i, (image, mask) in enumerate(tqdm(loader)):\n",
    "        flat_mask = mask.view(-1).to(device)\n",
    "        classes_present, class_counts = torch.unique(flat_mask, return_counts=True)\n",
    "        counts[classes_present] = class_counts\n",
    "\n",
    "    print(counts)\n",
    "    print(counts)\n",
    "    total = counts.sum().float()\n",
    "    print(total)\n",
    "\n",
    "    print(1 - counts.float() / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counts)\n",
    "total = counts.sum().float()\n",
    "print(total)\n",
    "\n",
    "1 - counts.float() / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.tensor([1,2,3,4,5])\n",
    "A.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "patients = pd.read_csv('data/hemorrhage_diagnosis_raw_ct.csv')\n",
    "h_start_idx = 2\n",
    "h_end_idx = 7\n",
    "N_negatives_val_test = 5\n",
    "\n",
    "hemorrhages = patients.drop(['SliceNumber', 'Fracture_Yes_No'], axis='columns')\n",
    "patients_grouped = patients.groupby(['PatientNumber']).agg({\n",
    "    'Intraventricular': 'sum',\n",
    "    'Intraparenchymal': 'sum',\n",
    "    'Subarachnoid': 'sum',\n",
    "    'Epidural': 'sum',\n",
    "    'Subdural': 'sum',\n",
    "    'No_Hemorrhage': 'sum',\n",
    "    'SliceNumber': 'max'\n",
    "    }).reset_index()\n",
    "\n",
    "#some_lesion = hemorrhages.iloc[:,h_end_idx:h_start_idx].sum(axis=1)\n",
    "#hemorrhages\n",
    "patients_with_hemorrhage = patients_grouped[patients_grouped['No_Hemorrhage'] != patients_grouped['SliceNumber'] ]\n",
    "patients_without_hemorrhage = patients_grouped[patients_grouped['No_Hemorrhage'] == patients_grouped['SliceNumber'] ]\n",
    "\n",
    "ivh = patients_with_hemorrhage[patients_with_hemorrhage.Intraventricular > 1]\n",
    "iph = patients_with_hemorrhage[patients_with_hemorrhage.Intraparenchymal > 1]\n",
    "sah = patients_with_hemorrhage[patients_with_hemorrhage.Subarachnoid > 1]\n",
    "edh = patients_with_hemorrhage[patients_with_hemorrhage.Epidural > 1]\n",
    "sdh = patients_with_hemorrhage[patients_with_hemorrhage.Subdural > 1]\n",
    "\n",
    "\n",
    "hems = ['Intraventricular', 'Intraparenchymal', 'Subarachnoid','Epidural','Subdural']\n",
    "\n",
    "test_patients = np.full((5,), fill_value=-1)\n",
    "val_patients = np.full((5,), fill_value=-1)\n",
    "for i, hemorrhage in enumerate(hems):\n",
    "    data_with_hemorrhage = np.array(patients_with_hemorrhage[patients_with_hemorrhage[hemorrhage] > 1].PatientNumber)\n",
    "    # print(data_with_hemorrhage)\n",
    "\n",
    "    taken = np.concatenate((test_patients, val_patients), axis=0)\n",
    "    free_to_take = data_with_hemorrhage[np.isin(data_with_hemorrhage, taken, invert=True)]\n",
    "\n",
    "    selected = np.random.choice(free_to_take, 2, replace=False)\n",
    "\n",
    "    test = selected[0]\n",
    "    val = selected[1]\n",
    "    test_patients[i] = test\n",
    "    val_patients[i] = val\n",
    "\n",
    "patients_numbers_without_hemorrhage = np.array(patients_without_hemorrhage.PatientNumber)\n",
    "test_negatives = np.random.choice(patients_numbers_without_hemorrhage, N_negatives_val_test, replace=False)\n",
    "patients_numbers_without_hemorrhage = patients_numbers_without_hemorrhage[np.isin(patients_numbers_without_hemorrhage, test_negatives, invert=True)]\n",
    "test_patients = np.concatenate((test_patients, test_negatives), axis=0)\n",
    "\n",
    "val_negatives = np.random.choice(patients_numbers_without_hemorrhage, N_negatives_val_test, replace=False)\n",
    "val_patients = np.concatenate((val_patients, val_negatives), axis=0)\n",
    "\n",
    "all_patients = patients_grouped.PatientNumber.to_numpy()\n",
    "taken = np.concatenate((test_patients, val_patients), axis=0)\n",
    "train_patients = all_patients[np.isin(all_patients, taken, invert=True)]\n",
    "\n",
    "print(test_patients, val_patients, train_patients)\n",
    "print(f\"in total: {test_patients.shape[0] + val_patients.shape[0] + train_patients.shape[0]}\")\n",
    "\n",
    "assert(np.intersect1d(val_patients, test_patients).size == 0)\n",
    "assert(np.intersect1d(val_patients, train_patients).size == 0)\n",
    "assert(np.intersect1d(test_patients, train_patients).size == 0)"
   ]
  }
 ]
}